{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5fd5b569-deaa-4577-bb74-b3ba0c7bd250",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c54ee550-87bc-4054-9d8a-7d922363ac07",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "import sys\n",
    "sys.path.append(\"/Workspace/Users/djaci.araujo@siemens.com/databricks-aisdk/mlops\")\n",
    "\n",
    "from si.preprocessing import (\n",
    "    positive_sum_of_changes,\n",
    "    negative_sum_of_changes,\n",
    "    SumColumnsTransformer,\n",
    ")\n",
    "import tsfresh.feature_extraction.feature_calculators as fc\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.sql.functions import pandas_udf\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from state_identifier.src.si.pipeline import (\n",
    "    WindowTransformer,\n",
    "    FeatureTransformer,\n",
    "    FillMissingValues,\n",
    "    back_propagate_labels,\n",
    ")\n",
    "from matplotlib import pyplot\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "\n",
    "@pandas_udf(DoubleType())\n",
    "def sumcols_udf(*cols: pd.Series) -> pd.Series:\n",
    "    # cols is a list of pandas Series; build a pandas DataFrame\n",
    "    data = pd.concat(cols, axis=1)\n",
    "    # transformer.transform returns 2D array; ravel to 1D\n",
    "    return pd.Series(transformer.transform(data).ravel())\n",
    "\n",
    "\n",
    "schema = \"aisdk\"\n",
    "table_name = \"state_identifier\"\n",
    "\n",
    "spark.sql(f\"USE {schema}\")\n",
    "\n",
    "df = spark.table(f\"{schema}.{table_name}\")\n",
    "# df.show()\n",
    "\n",
    "df = df.toPandas()\n",
    "\n",
    "input_columns = [\"ph1\", \"ph2\", \"ph3\"]\n",
    "# pdf = df[input_columns].toPandas()\n",
    "df[\"ph_sum\"] = SumColumnsTransformer().transform(df[input_columns].values).flatten()\n",
    "\n",
    "# df = df.withColumn(\"ph_sum\", sumcols_udf(*[F.col(c) for c in input_columns]))\n",
    "\n",
    "weighted_feature_list = [\n",
    "    (2, [fc.maximum, fc.minimum, fc.mean]),\n",
    "    (1, [fc.variance, fc.standard_deviation]),\n",
    "    (1, [fc.sum_values]),\n",
    "    (1, [fc.absolute_sum_of_changes]),\n",
    "    (1, [positive_sum_of_changes, negative_sum_of_changes]),\n",
    "    (\n",
    "        1,\n",
    "        [\n",
    "            fc.count_above_mean,\n",
    "            fc.longest_strike_above_mean,\n",
    "            fc.longest_strike_below_mean,\n",
    "        ],\n",
    "    ),\n",
    "]\n",
    "\n",
    "pipe = Pipeline(\n",
    "        [\n",
    "            (\n",
    "                \"preprocessing\",\n",
    "                Pipeline(\n",
    "                    [\n",
    "                        (\"fillmissing\", FillMissingValues(\"ffill\")),\n",
    "                        (\n",
    "                            \"summarization\",\n",
    "                            SumColumnsTransformer(),\n",
    "                        ),  # summarizes the variables into one variable\n",
    "                        (\n",
    "                            \"windowing\",\n",
    "                            WindowTransformer(window_size=300, window_step=300),\n",
    "                        ),\n",
    "                        (\n",
    "                            \"featurization\",\n",
    "                            FeatureTransformer(function_list=weighted_feature_list),\n",
    "                        ),\n",
    "                        (\"scaling\", MinMaxScaler(feature_range=(0, 1))),\n",
    "                    ]\n",
    "                ),\n",
    "            ),\n",
    "            (\"clustering\", KMeans(n_clusters=3, random_state=0)),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "x = df[input_columns].values  # transforming training data\n",
    "pipe.fit(x)\n",
    "x_classes = pipe.predict(x)\n",
    "df = back_propagate_labels(df, pipe[\"preprocessing\"], x_classes)\n",
    "\n",
    "colormap = {\n",
    "        -1: \"white\",\n",
    "        0: \"red\",\n",
    "        1: \"green\",\n",
    "        2: \"blue\",\n",
    "        3: \"orange\",\n",
    "        4: \"purple\",\n",
    "        5: \"yellow\",\n",
    "    }\n",
    "_, ax = pyplot.subplots(figsize=(24, 12))\n",
    "sns.scatterplot(\n",
    "    x=df.index, y=\"ph_sum\", data=df, hue=\"class\", palette=colormap, ax=ax\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "339e50ba-e5dc-4caa-b1f2-34f53d851054",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from mlflow.models import infer_signature\n",
    "\n",
    "mlflow.set_registry_uri(\"databricks\")\n",
    "registered_name = \"aisdk_model_state_identifier\"\n",
    "req_file = '/Workspace/Users/djaci.araujo@siemens.com/databricks-aisdk/mlops/state_identifier/src/requirements.txt'\n",
    "\n",
    "# X_example = df[input_columns].head(5)\n",
    "# sig = infer_signature(X_example, pipe.predict(X_example))\n",
    "\n",
    "with mlflow.start_run():\n",
    "    mlflow.sklearn.log_model(\n",
    "        sk_model=pipe,\n",
    "        artifact_path=\"model\",\n",
    "        registered_model_name=registered_name,\n",
    "        # input_example=X_example.iloc[:1],\n",
    "        # signature=sig,\n",
    "        pip_requirements=req_file,\n",
    "        # If the pipeline references custom classes (e.g., SumColumnsTransformer),\n",
    "        # make sure that module is importable at serve time:\n",
    "        # code_paths=[\"/Workspace/Repos/you/project/src\"]  # or infer_code_paths=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "79a3336a-01b0-424f-8e17-d907974c2459",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "02-state_identifier-train",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
