{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c54ee550-87bc-4054-9d8a-7d922363ac07",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from si.preprocessing import (\n",
    "    positive_sum_of_changes,\n",
    "    negative_sum_of_changes,\n",
    "    SumColumnsTransformer,\n",
    ")\n",
    "from si.pipeline import (\n",
    "    WindowTransformer,\n",
    "    FeatureTransformer,\n",
    "    FillMissingValues,\n",
    "    back_propagate_labels,\n",
    ")\n",
    "import tsfresh.feature_extraction.feature_calculators as fc\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.sql.functions import pandas_udf\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from matplotlib import pyplot\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "\n",
    "schema = \"aisdk\"\n",
    "table_name = \"state_identifier\"\n",
    "\n",
    "spark.sql(f\"USE {schema}\")\n",
    "\n",
    "df = spark.table(f\"{schema}.{table_name}\")\n",
    "# df.show()\n",
    "\n",
    "df = df.toPandas()\n",
    "\n",
    "input_columns = [\"ph1\", \"ph2\", \"ph3\"]\n",
    "df[\"ph_sum\"] = SumColumnsTransformer().transform(df[input_columns].values).flatten()\n",
    "\n",
    "\n",
    "weighted_feature_list = [\n",
    "    (2, [fc.maximum, fc.minimum, fc.mean]),\n",
    "    (1, [fc.variance, fc.standard_deviation]),\n",
    "    (1, [fc.sum_values]),\n",
    "    (1, [fc.absolute_sum_of_changes]),\n",
    "    (1, [positive_sum_of_changes, negative_sum_of_changes]),\n",
    "    (\n",
    "        1,\n",
    "        [\n",
    "            fc.count_above_mean,\n",
    "            fc.longest_strike_above_mean,\n",
    "            fc.longest_strike_below_mean,\n",
    "        ],\n",
    "    ),\n",
    "]\n",
    "\n",
    "pipe = Pipeline(\n",
    "        [\n",
    "            (\n",
    "                \"preprocessing\",\n",
    "                Pipeline(\n",
    "                    [\n",
    "                        (\"fillmissing\", FillMissingValues(\"ffill\")),\n",
    "                        (\"summarization\", SumColumnsTransformer()),\n",
    "                        (\"windowing\", WindowTransformer(window_size=300, window_step=300)),\n",
    "                        (\"featurization\", FeatureTransformer(function_list=weighted_feature_list)),\n",
    "                        (\"scaling\", MinMaxScaler(feature_range=(0, 1))),\n",
    "                    ]\n",
    "                ),\n",
    "            ),\n",
    "            (\"clustering\", KMeans(n_clusters=3, random_state=0)),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "x = df[input_columns].values  # transforming training data\n",
    "pipe.fit(x)\n",
    "x_classes = pipe.predict(x)\n",
    "df = back_propagate_labels(df, pipe[\"preprocessing\"], x_classes)\n",
    "\n",
    "colormap = {\n",
    "        -1: \"white\",\n",
    "        0: \"red\",\n",
    "        1: \"green\",\n",
    "        2: \"blue\",\n",
    "        3: \"orange\",\n",
    "        4: \"purple\",\n",
    "        5: \"yellow\",\n",
    "    }\n",
    "_, ax = pyplot.subplots(figsize=(24, 12))\n",
    "sns.scatterplot(\n",
    "    x=df.index, y=\"ph_sum\", data=df, hue=\"class\", palette=colormap, ax=ax\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "339e50ba-e5dc-4caa-b1f2-34f53d851054",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from mlflow.models import infer_signature\n",
    "\n",
    "mlflow.set_registry_uri(\"databricks\")\n",
    "registered_name = \"aisdk_model_state_identifier\"\n",
    "req_file = 'requirements.txt'\n",
    "\n",
    "input_example = df[input_columns].head(300)\n",
    "\n",
    "# predictions for signature inference\n",
    "pred_array = pipe.predict(input_example.values)\n",
    "pred_df = pd.DataFrame(pred_array, columns=[\"prediction\"])\n",
    "\n",
    "# infer signature\n",
    "signature = infer_signature(input_example, pred_df)\n",
    "\n",
    "with mlflow.start_run():\n",
    "    mlflow.sklearn.log_model(\n",
    "        sk_model=pipe,\n",
    "        artifact_path=\"model\",\n",
    "        registered_model_name=registered_name,\n",
    "        input_example=input_example,\n",
    "        signature=signature,\n",
    "        pip_requirements=req_file\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "79a3336a-01b0-424f-8e17-d907974c2459",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "02-state_identifier-train",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
